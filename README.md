# Paper notes

Notes from papers I'm reading, ordered by topic and chronologically.

## NLP

1. Conditional Random Fields: probabilistic models for segmenting and labeling sequence data, Lafferty et al, 2001 [[Paper](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)] [[Notes](-2017/0106.md)] [\#nlp](#nlp) [\#architectures](#architectures)
2. Introduction to the CoNLL-2003 shared task: language-independent named entity recognition, Sang et al., 2003 [[Paper](https://dl.acm.org/citation.cfm?id=1119195)] [[Notes](-2017/0306.md)] [\#nlp](#nlp) [\#datasets](#datasets)
3. Bidirectional LSTM-CRF Models for sequence tagging, Huang et al., 2015 [[Paper](https://arxiv.org/abs/1508.01991)] [[Notes](-2017/1508.01991.md)] [\#nlp](#nlp) [\#architectures](#architectures)
4. Neural Machine Translation of Rare Words with Subword Units, Sennrich et al., 2015 [[Paper](https://arxiv.org/abs/1508.07909)] [[Notes](-2017/1508.07909.md)] [\#nlp](#nlp)
5. Neural Architectures for Named Entity Recognition, Lample et al., 2016 [[Paper](https://www.aclweb.org/anthology/N16-1030)] [[Notes](-2017/1606.1030.md)] [\#nlp](#nlp) [\#architectures](#architectures) [\#NER](#ner)
6. Named Entity Recognition with Bidirectional LSTM-CNNs, Chiu et al., 2016 [[Paper](https://www.aclweb.org/anthology/Q16-1026)] [[Notes](-2017/1607.1026.md)] [\#nlp](#nlp) [\#architectures](#architectures)
7. Semi-supervised sequence tagging with bidirectional language models, Peters et al., 2017 [[Paper](https://arxiv.org/abs/1705.00108)] [[Notes](-2017/1705.00108.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
8. **Attention is all you need**, Vaswani et al., 2018 [[Paper](https://arxiv.org/abs/1706.03762)] [[Notes](-2017/1706.03762.md)] [\#nlp](#nlp) [\#architectures](#architectures)
9. Deep contextualized word representations, Peters et al., 2018 [[Paper](https://arxiv.org/abs/1802.05365)] [[Notes](2018/1802.05365.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
10. Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable, Hangya et al., 2018 [[Paper](https://www.aclweb.org/anthology/P18-1075)] [[Notes](2018/1807.1075.md)] [\#nlp](#nlp)
11. A Named Entity Recognition Shootout for German, Riedl and Padó, 2018 [[Paper](https://www.aclweb.org/anthology/P18-2020)] [[Notes](2018/1807.2020.md)] [\#nlp](#nlp) [\#NER](#ner) [\#datasets](#datasets)
12. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference, Zellers et al., 2018 [[Paper](https://arxiv.org/abs/1808.05326)] [[Notes](2018/1808.05326.md)] [\#nlp](#nlp) [\#datasets](#datasets)
13. Dissecting contextual word embeddings: architecture and representation, Peters et al., 2018 [[Paper](https://arxiv.org/abs/1808.08949)] [[Notes](2018/1808.08949.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
14. Contextual string embeddings for sequence labeling, Akbik et al., 2018 [[Paper](https://alanakbik.github.io/papers/coling2018.pdf)] [[Notes](2018/1808.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
15. Targeted synctactic evaluation of language models, Marvin and Linzen, 2018 [[Paper](https://arxiv.org/abs/1808.09031)] [[Notes](2018/1808.09031.md)] [\#nlp](#nlp) [\#linguistics](#linguistics)
16. **BERT: Pre-training of deep bidirectional transformers for language understanding**, Devlin et al., 2018 [[Paper](https://arxiv.org/abs/1810.04805)] [[Notes](2018/1810.04805.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
17. Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia, Yamada et al., 2018 [[Paper](https://arxiv.org/abs/1812.06280)] [[Notes](2018/1812.06280.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
18. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference., McCoy et al., 2019 [[Paper](https://arxiv.org/abs/1902.01007)] [[Notes](2019/1902.01007.md)] [\#nlp](#nlp) [\#linguistics](#linguistics) [\#datasets](#datasets)
19. Linguistic Knowledge and Transferability of Contextual Representations, Liu et al., 2019 [[Paper](https://arxiv.org/abs/1903.08855)] [[Notes](2019/1903.08855.md)] [\#nlp](#nlp)
20. What do you learn from context? Probing for sentence structure in contextualized word representations, Tenney et al., 2019 [[Paper](https://openreview.net/forum?id=SJzSgnRcKX)] [[Notes](2019/1905.md)] [\#nlp](#nlp)
21. HellaSwag: Can a Machine Really Finish Your Sentence?, Zellers et al., 2019 [[Paper](https://arxiv.org/abs/1905.07830)] [[Notes](2019/1905.07830.md)] [\#nlp](#nlp) [\#datasets](#datasets)
22. Flair: an easy-to-use framework for stat-of-the-art NLP [[Paper](https://www.aclweb.org/anthology/N19-4010)] [[Notes](2019/1906.4010.md)] [\#nlp](#nlp) [\#embeddings](#frameworks)
23. Towards Robust Named Entity Recognition for Historic German, Schweter et al., 2019 [[Paper](https://arxiv.org/abs/1906.07592)] [[Notes](2019/1906.07592.md)] [\#nlp](#nlp) [\#NER](#ner)
24. XLNet: generalized autoregressive pretraining for language understanding, Yang et al., 2019 [[Paper](https://arxiv.org/abs/1906.08237)] [[Notes](2019/1906.08237.md)] [\#nlp](#nlp) [\#architectures](#architectures)
25. R-Transformer: Recurrent Neural Network Enhanced Transformer, Wang et al., 2019 [[Paper](https://arxiv.org/abs/1907.05572)] [[Notes](2019/1907.05572.md)] [\#nlp](#nlp) [\#architectures](#architectures)
26. Probing Neural Network Comprehension of Natural Language Arguments, Nivel and Kao, 2019 [[Paper](https://arxiv.org/abs/1907.07355)] [[Notes](2019/1907.07355.md)] [\#nlp](#nlp) [\#datasets](#datasets)
27. Language Models as Knowledge Bases?, Petroni et al., 2019 [[Paper](https://arxiv.org/abs/1909.01066)] [[Notes](2019/1909.01066.md)] [\#nlp](#nlp) [\#linguistics](#linguistics)
28. HuggingFace's Transformers: State-of-the-art Natural Language Processing, Wolf et al., 2019 [[Paper](https://arxiv.org/abs/1910.03771)] [[Notes](2019/1910.03771.md)] [\#nlp](#nlp) [\#frameworks](#frameworks)
29. Evaluating the Factual Consistency of Abstractive Text Summarization, Kryscinski et al., 2019 [[Paper](https://arxiv.org/abs/1910.12840)] [[Notes](2019/1910.12840.md)] [\#nlp](#nlp) [\#text-summarization](#text-summarization)
30. Generalization through Memorization: Nearest Neighbor Language Models, Khandelwal et al., 2019 [[Paper](https://arxiv.org/abs/1911.001723)] [[Notes](2019/1911.001723.md)] [\#nlp](#nlp) [\#architectures](#architectures)
31. BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA, Poerner et al., 2019 [[Paper](https://arxiv.org/abs/1911.03681)] [[Notes](2019/1911.03681.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
32. Single Headed Attention RNN: Stop Thinking With Your Head, Merity, 2019 [[Paper](https://arxiv.org/abs/1911.11423)] [[Notes](2019/1911.11423.md)] [\#nlp](#nlp) [\#architectures](#architectures)
33. What’s Going On in Neural Constituency Parsers? An Analysis, Gaddy et al., 2018 [[Paper](https://arxiv.org/abs/1804.07853)] [[Notes](2018/1804.07853.md)] [\#nlp](#nlp)
34. BPE-Dropout: simple and effective subword regularization, Provilkov et al., 2019 [[Paper](https://arxiv.org/abs/1910.13267)] [[Notes](2019/1910.13267.md)] [\#nlp](#nlp)
35. Pre-trained Models for Natural Language Processing: A Survey, Qiu et al., 2020 [[Paper](https://arxiv.org/abs/2003.08271)] [\#nlp](#nlp)

### Embeddings

1. Semi-supervised sequence tagging with bidirectional language models, Peters et al., 2017 [[Paper](https://arxiv.org/abs/1705.00108)] [[Notes](-2017/1705.00108.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
2. Deep contextualized word representations, Peters et al., 2018 [[Paper](https://arxiv.org/abs/1802.05365)] [[Notes](2018/1802.05365.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
3. Dissecting contextual word embeddings: architecture and representation, Peters et al., 2018 [[Paper](https://arxiv.org/abs/1808.08949)] [[Notes](2018/1808.08949.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
4. BERT: Pre-training of deep bidirectional transformers for language understanding, Devlin et al., 2018 [[Paper](https://arxiv.org/abs/1810.04805)] [[Notes](2018/1810.04805.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
5. Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia, Yamada et al., 2018 [[Paper](https://arxiv.org/abs/1812.06280)] [[Notes](2019/1812.06280.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)
6. BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA, Poerner et al., 2019 [[Paper](https://arxiv.org/abs/1911.03681)] [[Notes](2019/1911.03681.md)] [\#nlp](#nlp) [\#embeddings](#embeddings)

### Architectures

1. Conditional Random Fields: probabilistic models for segmenting and labeling sequence data, Lafferty et al, 2001 [[Paper](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1162&context=cis_papers)] [[Notes](-2017/0106.md)] [\#nlp](#nlp) [\#architectures](#architectures)
2. Bidirectional LSTM-CRF Models for sequence tagging, Huang et al., 2015 [[Paper](https://arxiv.org/abs/1508.01991)] [[Notes](-2017/1508.01991.md)] [\#nlp](#nlp) [\#architectures](#architectures)
3. Neural Architectures for Named Entity Recognition, Lample et al., 2016 [[Paper](https://www.aclweb.org/anthology/N16-1030)] [[Notes](-2017/1606.1030.md)] [\#nlp](#nlp) [\#architectures](#architectures) [\#NER](#ner)
4. Named Entity Recognition with Bidirectional LSTM-CNNs, Chiu et al., 2016 [[Paper](https://www.aclweb.org/anthology/16-1026)] [[Notes](-2017/1607.1026.md)] [\#nlp](#nlp) [\#architectures](#architectures)
5. Attention is all you need, Vaswani et al., 2018 [[Paper](https://arxiv.org/abs/1706.03762)] [[Notes](-2017/1706.03762.md)] [\#nlp](#nlp) [\#architectures](#architectures)
6. Reasoning with Sarcasm by Reading In-between, Tay et al., 2018 [[Paper](https://www.aclweb.org/anthology/P18-1093/)] [[Notes](2018/1807.1093.md)] [\#sarcasm-detection](#sarcasm-detection) [\#architectures](#architectures)
7. XLNet: generalized autoregressive pretraining for language understanding, Yang et al., 2019 [[Paper](https://arxiv.org/abs/1906.08237)] [[Notes](2019/1906.08237.md)] [\#nlp](#nlp) [\#architectures](#architectures)
8. R-Transformer: Recurrent Neural Network Enhanced Transformer, Wang et al., 2019 [[Paper](https://arxiv.org/abs/1907.05572)] [[Notes](2019/1907.05572.md)] [\#nlp](#nlp) [\#architectures](#architectures)
9. Generalization through Memorization: Nearest Neighbor Language Models, Khandelwal et al., 2019 [[Paper](https://arxiv.org/abs/1911.001723)] [[Notes](2019/1911.001723.md)] [\#nlp](#nlp) [\#architectures](#architectures)
10. Single Headed Attention RNN: Stop Thinking With Your Head, Merity, 2019 [[Paper](https://arxiv.org/abs/1911.11423)] [[Notes](2019/1911.11423.md)] [\#nlp](#nlp) [\#architectures](#architectures)
11. A Transformer-based approach to Irony and Sarcasm detection, Potamias et al., 2019 [[Paper](https://arxiv.org/abs/1911.10401)] [[Notes](2019/1911.10401.md)] [\#sarcasm-detection](#sarcasm-detection) [\#architecture](#architecture)

### Frameworks

1. Flair: an easy-to-use framework for stat-of-the-art NLP [[Paper](https://www.aclweb.org/anthology/N19-4010)] [[Notes](2019/1906.4010.md)] [\#nlp](#nlp) [\#frameworks](#frameworks)
2. HuggingFace's Transformers: State-of-the-art Natural Language Processing, Wolf et al., 2019 [[Paper](https://arxiv.org/abs/1910.03771)] [[Notes](2019/1910.03771.md)] [\#nlp](#nlp) [\#frameworks](#frameworks)
3. Selective Brain Damage: Measuring the Disparate Impact of Model Pruning, Hooker et al., 2019 [[Paper](https://arxiv.org/abs/1911.05248)] [[Notes](2019/1911.05248.md)] [\#frameworks](#frameworks)

### Datasets

1. Introduction to the CoNLL-2003 shared task: language-independent named entity recognition, Sang et al., 2003 [[Paper](https://dl.acm.org/citation.cfm?id=1119195)] [[Notes](-2017/0306.md)] [\#nlp](#nlp) [\#datasets](#datasets)
2. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference, Zellers et al., 2018 [[Paper](https://arxiv.org/abs/1808.05326)] [[Notes](2018/1808.05326.md)] [\#nlp](#nlp) [\#datasets](#datasets)
3. A Named Entity Recognition Shootout for German, Riedl and Padó, 2018 [[Paper](https://www.aclweb.org/anthology/P18-2020)] [[Notes](2018/1807.2020.md)] [\#nlp](#nlp) [\#NER](#ner) [\#datasets](#datasets)
4. Probing Neural Network Comprehension of Natural Language Arguments, Nivel and Kao, 2019 [[Paper](https://arxiv.org/abs/1907.07355)] [[Notes](2019/1907.07355.md)] [\#nlp](#nlp) [\#datasets](#datasets)
5. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference., McCoy et al., 2019 [[Paper](https://arxiv.org/abs/1902.01007)] [[Notes](2019/1902.01007.md)] [\#nlp](#nlp) [\#linguistics](#linguistics) [\#datasets](#datasets)
6. UR-FUNNY: A Multimodal Language Dataset for Understanding Humor, Hasan et al., 2019 [[Paper](https://arxiv.org/abs/1904.06618)] [[Notes](2019/1904.06618.md)] [\#sarcasm-detection](#sarcasm-detection) [\#datasets](#datasets)
7. HellaSwag: Can a Machine Really Finish Your Sentence?, Zellers et al., 2019 [[Paper](https://arxiv.org/abs/1905.07830)] [[Notes](2019/1905.07830.md)] [\#nlp](#nlp) [\#datasets](#datasets)
8. Sentiment analysis is not solved! Assessing and probing sentiment classification, Barnes et al., 2019 [[Paper](https://arxiv.org/abs/1906.05887)] [[Notes](2019/1906.05887.md)] [\#nlp](#nlp) [\#datasets](#datasets)
9. Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model, Cai et al., 2019 [[Paper](https://www.aclweb.org/anthology/P19-1239/)] [[Notes](2019/1907.1239.md)] [\#sarcasm-detection](#sarcasm-detection) [\#datasets](#datasets)
10. Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper), Castro et al., 2019 [[Paper](https://www.aclweb.org/anthology/P19-1455/)] [[Notes](2019/1907.1455.md)] [\#sarcasm-detection](#sarcasm-detection) [\#datasets](#datasets)
11. iSarcasm: A Dataset of Intended Sarcasm, Oprea et al., 2019 [[Paper](https://arxiv.org/abs/1911.03123)] [[Notes](2019/1911.03123.md)] [\#datasets](#dataset) [\#sarcasm-detection](#sarcasm-detection)

### NER

1. Introduction to the CoNLL-2003 shared task: language-independent named entity recognition, Sang et al., 2003 [[Paper](https://dl.acm.org/citation.cfm?id=1119195)] [[Notes](-2017/0306.md)] [\#nlp](#nlp) [\#datasets](#datasets) [\#NER](#ner)
2. Neural Architectures for Named Entity Recognition, Lample et al., 2016 [[Paper](https://www.aclweb.org/anthology/N16-1030)] [[Notes](-2017/1606.1030.md)] [\#nlp](#nlp) [\#architectures](#architectures) [\#NER](#ner)
3. Named Entity Recognition with Bidirectional LSTM-CNNs, Chiu et al., 2016 [[Paper](https://www.aclweb.org/anthology/Q16-1026)] [[Notes](-2017/1607.1026.md)] [\#nlp](#nlp) [\#architectures](#architectures) [\#NER](#ner)
4. Towards Robust Named Entity Recognition for Historic German, Schweter et al., 2019 [[Paper](https://arxiv.org/abs/1906.07592)] [[Notes](2019/1906.07592.md)] [\#nlp](#nlp) [\#NER](#ner)
5. A Named Entity Recognition Shootout for German, Riedl and Padó, 2018 [[Paper](https://www.aclweb.org/anthology/P18-2020)] [[Notes](2018/1807.2020.md)] [\#nlp](#nlp) [\#NER](#ner) [\#datasets](#datasets)

### Sarcasm detection

[summary](sarcasm_detection.md)

1. Sarcasm Detection on Twitter: A Behavioral Modeling Approach, Rajadesingan et al., 2015 [[Paper](https://dl.acm.org/citation.cfm?id=2685316)] [[Notes](-2017/1502.md)] [\#sarcasm-detection](#sarcasm-detection)
2. Contextualized Sarcasm Detection on Twitter, Bamman and Smith, 2015 [[Paper](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM15/paper/viewPaper/10538)] [[Notes](-2017/1504.md)] [\#sarcasm-detection](#sarcasm-detection)
3. Harnessing Context Incongruity for Sarcasm Detection, Joshi et al., 2015 [[Paper](https://www.aclweb.org/anthology/P15-2124/)] [[Notes](-2017/1507.2124.md)] [\#linguistics](#linguistics) [\#sarcasm-detection](#sarcasm-detection)
4. Automatic Sarcasm Detection: A Survey, Joshi et al., 2017 [[Paper](https://dl.acm.org/citation.cfm?id=3124420)] [[Notes](-2017/1602.03426.md)] [\#sarcasm-detection](#sarcasm-detection)
5. Detecting Sarcasm is Extremely Easy ;-), Parde and Nielsen, 2018 [[Paper](https://www.aclweb.org/anthology/W18-1303/)] [[Notes](2018/1806.1303.md)] [\#sarcasm-detection](#sarcasm-detection)
6. CASCADE: Contextual Sarcasm Detection in Online Discussion Forums, Hazarika et al., 2018 [[Paper](https://arxiv.org/abs/1805.06413)] [[Notes](2018/1805.06413.md)] [\#sarcasm-detection](#sarcasm-detection)
7. Reasoning with Sarcasm by Reading In-between, Tay et al., 2018 [[Paper](https://www.aclweb.org/anthology/P18-1093/)] [[Notes](2018/1807.1093.md)] [\#sarcasm-detection](#sarcasm-detection) [\#architectures](#architectures)
8. Tweet Irony Detection with Densely Connected LSTM and Multi-task Learning, Wu et al., 2018 [[Paper](https://www.aclweb.org/anthology/S18-1006/)] [[Notes](2018/1806.1006.md)] [\#sarcasm-detection](#sarcasm-detection)
9. UR-FUNNY: A Multimodal Language Dataset for Understanding Humor, Hasan et al., 2019 [[Paper](https://arxiv.org/abs/1904.06618)] [[Notes](2019/1904.06618.md)] [\#sarcasm-detection](#sarcasm-detection) [\#datasets](#datasets)
10. Exploring Author Context for Detecting Intended vs Perceived Sarcasm, Oprea and Magdy, 2019 [[Paper](https://www.aclweb.org/anthology/P19-1275/)] [[Notes](2019/1907.1275.md)] [\#sarcasm-detection](#sarcasm-detection)
11. Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper), Castro et al., 2019 [[Paper](https://www.aclweb.org/anthology/P19-1455/)] [[Notes](2019/1907.1455.md)] [\#sarcasm-detection](#sarcasm-detection) [\#datasets](#datasets)
12. Multi-Modal Sarcasm Detection in Twitter with Hierarchical Fusion Model, Cai et al., 2019 [[Paper](https://www.aclweb.org/anthology/P19-1239/)] [[Notes](2019/1907.1239.md)] [\#sarcasm-detection](#sarcasm-detection) [\#datasets](#datasets)
13. A2Text-Net: A Novel Deep Neural Network for Sarcasm Detection, Liu et al., 2019 [[Paper](https://www.researchgate.net/profile/Liyuan_Liu23/publication/337425314_A2Text-Net_A_Novel_Deep_Neural_Network_for_Sarcasm_Detection/links/5dd6bd1d458515dc2f41db91/A2Text-Net-A-Novel-Deep-Neural-Network-for-Sarcasm-Detection.pdf)] [[Notes](2019/1912.33742.md)] [\#sarcasm-detection](#sarcasm-detection)
14. Sarcasm detection in tweets, Rajagopalan et al., 2019 [[Paper](https://jadhosn.github.io/projects/CSE575_FinalReport-SarcasmDetection.pdf)] [[Notes](2019/1911.575.md)] [\#sarcasm-detection](#sarcasm-detection)
15. A Transformer-based approach to Irony and Sarcasm detection, Potamias et al., 2019 [[Paper](https://arxiv.org/abs/1911.10401)] [[Notes](2019/1911.10401.md)] [\#sarcasm-detection](#sarcasm-detection) [\#architecture](#architecture)
16. Deep and dense sarcasm detection, Pelser et al., 2019 [[Paper](https://arxiv.org/abs/1911.07474)] [[Notes](2019/1911.07474.md)] [\#sarcasm-detection](#sarcasm-detection)
17. iSarcasm: A Dataset of Intended Sarcasm, Oprea et al., 2019 [[Paper](https://arxiv.org/abs/1911.03123)] [[Notes](2019/1911.03123.md)] [\#datasets](#dataset) [\#sarcasm-detection](#sarcasm-detection)

### Text summarization

1. Evaluating the Factual Consistency of Abstractive Text Summarization, Kryscinski et al., 2019 [[Paper](https://arxiv.org/abs/1910.12840)] [[Notes](2019/1910.12840.md)] [\#nlp](#nlp) [\#text-summarization](#text-summarization)

---

## Reinforcement learning

1. Theory of Minds: Understanding Behavior in Groups Through Inverse Planning, Shum et al., 2019 [[Paper](https://arxiv.org/abs/1901.06085)] [[Notes](2019/1901.06085.md)] [\#reinforcement-learning](#reinforcement-learning) [\#social-sciences](#social-sciences)
2. The Hanabi Challenge: A New Frontier for AI Research, Bard et al., 2019 [[Paper](https://arxiv.org/abs/1902.00506)] [[Notes](2019/1902.00506.md)] [\#reinforcement-learning](#reinforcement-learning)
3. Mastering Atari, Go, Chess and Shogi by Planning with a learned model, Schrittwieser et al., 2019 [[Paper](https://arxiv.org/abs/1911.08265)] [[Notes](2019/1911.08265.md)] [\#reinforcement-learning](#reinforcement-learning)

---

## Computer vision

1. Cubic Stylization, Derek Liu and Jacobson, 2019 [[Paper](https://arxiv.org/abs/1910.02926)] [[Notes](2019/1910.02926.md)] [\#computer-vision](#computer-vision)

---

## Linguistics

1. Moving beyond the plateau: from lower-intermediate to upper-intermediate, Richards, 2015 [[Paper](https://www.cambridge.org/elt/blog/2015/08/26/moving-beyond-plateau-lower-upper-intermediate/)] [[Notes](-2017/1508.md)] [\#linguistics](#linguistics)
2. Harnessing Context Incongruity for Sarcasm Detection, Joshi et al., 2015 [[Paper](https://www.aclweb.org/anthology/P15-2124/)] [[Notes](-2017/1507.2124.md)] [\#linguistics](#linguistics) [\#sarcasm-detection](#sarcasm-detection)
3. A Trainable Spaced Repetition Model for Language Learning, Settles and Meeder, 2016 [[Paper](https://www.aclweb.org/anthology/P16-1174/)] [[Notes](-2017/1608.1174.md)] [\#linguistics](#linguistics)
4. Targeted synctactic evaluation of language models, Marvin and Linzen, 2018 [[Paper](https://arxiv.org/abs/1808.09031)] [[Notes](2018/1808.09031.md)] [\#nlp](#nlp) [\#linguistics](#linguistics)
5. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference., McCoy et al., 2019 [[Paper](https://arxiv.org/abs/1902.01007)] [[Notes](2019/1902.01007.md)] [\#nlp](#nlp) [\#linguistics](#linguistics) [\#datasets](#datasets)
6. Language Models as Knowledge Bases?, Petroni et al., 2019 [[Paper](https://arxiv.org/abs/1909.01066)] [[Notes](2019/1909.01066.md)] [\#nlp](#nlp) [\#linguistics](#linguistics)
7. Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche, Coupé et al., 2019 [[Paper](https://advances.sciencemag.org/content/5/9/eaaw2594)] [[Notes](2019/190904.md)] [\#linguistics](#linguistics) [\#social-sciences](#social-sciences)
8. My English sounds better than yours: Second language learners perceive their own accent as better than that of their peers, Mittlerer et al., 2020 [[Paper](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0227643)] [[Notes](2020/2001.0227643.md)] [\#linguistics](#linguistics)

---

## Social sciences

1. How much does education improve intelligence? A meta-analysis, Ritchie et al., 2017 [[Paper](https://journals.sagepub.com/doi/abs/10.1177/0956797618774253)] [[Notes](-2017/1711.md)] [\#social-sciences](#social-sciences)
2. Theory of Minds: Understanding Behavior in Groups Through Inverse Planning, Shum et al., 2019 [[Paper](https://arxiv.org/abs/1901.06085)] [[Notes](2019/1901.06085.md)] [\#reinforcement-learning](#reinforcement-learning) [\#social-sciences](#social-sciences)
3. Fake news game confers psychological resistance against online misinformation, Roozenbeek and van der Linden, 2019 [[Paper](https://www.nature.com/articles/s41599-019-0279-9)] [[Notes](2019/1908.md)] [\#social-sciences](#social-sciences) [\#humanities](#humanities)
4. Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche, Coupé et al., 2019 [[Paper](https://advances.sciencemag.org/content/5/9/eaaw2594)] [[Notes](2019/190904.md)] [\#linguistics](#linguistics) [\#social-sciences](#social-sciences)
5. Kids these days: Why the youth of today seem lacking, Protzko and Schooler, 2019 [[Paper](https://advances.sciencemag.org/content/5/10/eaav5916)] [[Notes](2019/1910.5916.md)] [\#social-sciences](#social-sciences)

---

## Humanities

1. Fake news game confers psychological resistance against online misinformation, Roozenbeek and van der Linden, 2019 [[Paper](https://www.nature.com/articles/s41599-019-0279-9)] [[Notes](2019/1908.md)] [\#social-sciences](#social-sciences) [\#humanities](#humanities)

---

## Physics

1. First-order transition in a model of prestige bias, Skinner, 2019 [[Paper](https://arxiv.org/abs/1910.05813)] [[Notes](2019/1910.05813.md)] [\#physics](#physics)

---

## Neuroscience

1. A deep learning framework for neuroscience, Richard et al., 2019 [[Paper](https://www.nature.com/articles/s41593-019-0520-2)] [[Notes](2019/1911.41503.md)] [\#neuroscience](#neuroscience)

---

## Algorithms

1. Replace or Retrieve Keywords In Documents At Scale, Singh, 2017 [[Paper](https://arxiv.org/abs/1711.00046)] [[Notes](-2017/1711.00046.md)] [\#algorithms](#algorithms)
