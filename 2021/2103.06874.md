# CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation, Clark et al., 2021

## [Paper](https://arxiv.org/abs/2103.06874), Tags: \#nlp

CANINE (character architecture with no tokenization in neural encoders) is a neural encoder that operates directly on character sequences, without explicit tokenization or vocabulary, and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. 

CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context.

Tokenizers are usually either language-specific which is costly, requiring manual feature engineering and linguistic expertise, or data-driven algorithms such as BPE, WordPiece and SentencePiece, which are less brittle and easier to scale but too simplistic to properly handle the wide range of linguistic phenomena that can't be captured by mere string-splitting.

Giving character inputs to transformer models have several problems, they make the models slower and have poorer performance. CANINE enables a tokenization-free modeling that overcomes these obstacles. Inputs to CANINE are sequences of Unicode characters.

Subword models still struggle on informal text, typos, spelling variation, transliteration or emoji.

CANINE embeds the Unicode integers using multiple hash functions, a vocabulary-free generalization of the word hash embedding trick. This allows CANINE to represent all 143k Unicode characters with a relatively small number of parameters. Because the model suppots all endpoints, representations can be learnt during fine-tuning for characters never seen during pre-training.