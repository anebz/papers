# Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT, Chronopoulou et al., 2020

## [Paper](https://www.aclweb.org/anthology/2020.emnlp-main.214/), [Code](https://github.com/alexandra-chron/relm_unmt), Tags: \#nlp, \#machine-translation

The reused monolingual LM is fine-tuned on both languages and then used to initialize a UNMT model. We propose a new vocabulary extension method, RE-LM, that outperforms a competitive cross-lingual pretraining model and also improves translations on a low-resource supervised setup.
